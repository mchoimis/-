{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import time\n",
    "\n",
    "PYTHONHASHSEED=0\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy 1.1.0\n",
      "numpy 1.13.3\n",
      "matplotlib 1.5.3\n",
      "pandas 0.23.0\n",
      "sklearn 0.19.1\n",
      "h5py 2.7.1\n",
      "tensorflow 1.10.0\n",
      "keras 2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import matplotlib\n",
    "import pandas\n",
    "import sklearn\n",
    "# import pydot\n",
    "import h5py\n",
    "\n",
    "import tensorflow\n",
    "# import keras\n",
    "\n",
    "print('scipy ' + scipy.__version__)\n",
    "print('numpy ' + numpy.__version__)\n",
    "print('matplotlib ' + matplotlib.__version__)\n",
    "print('pandas ' + pandas.__version__)\n",
    "print('sklearn ' + sklearn.__version__)\n",
    "print('h5py ' + h5py.__version__)\n",
    "\n",
    "print('tensorflow ' + tensorflow.__version__)\n",
    "print('keras ' + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_pickle('.pkl')\n",
    "test_dataset = pd.read_pickle('.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.index = range(len(train_dataset.index))\n",
    "test_dataset.index = range(len(test_dataset.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 변환 및 기초작업\n",
    "# 결측치 중 nan 이면 0으로 대체\n",
    "train_dataset.iloc[:, 0:20] = train_dataset.iloc[:, 0:20].fillna(0)\n",
    "train_dataset.iloc[:, 53:73] = train_dataset.iloc[:, 53:73].fillna(0)\n",
    "\n",
    "# 남은 변수의 경우 NA 경우 제거\n",
    "train_dataset = train_dataset.dropna()\n",
    "\n",
    "# 동일작업 test set에도 실시\n",
    "test_dataset.iloc[:, 0:20] = test_dataset.iloc[:, 0:20].fillna(0)\n",
    "test_dataset.iloc[:, 53:73] = test_dataset.iloc[:, 53:73].fillna(0)\n",
    "test_dataset = test_dataset.dropna()\n",
    "\n",
    "# train_dataset.shape # (203728, 35)\n",
    "# test_dataset.shape  # (11518, 35)\n",
    "\n",
    "# label 과 feature 분리\n",
    "train_labels = train_dataset[train_dataset.columns[53:73]].copy()\n",
    "test_labels = test_dataset[test_dataset.columns[53:73]].copy()\n",
    "\n",
    "train_dataset = train_dataset.drop(train_dataset.columns[53:73], 1)\n",
    "test_dataset = test_dataset.drop(test_dataset.columns[53:73], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. train set\n",
    "# 추후 1개월에 대해 카드 이력이 없는 사람을 제거\n",
    "drop_y = train_labels.loc[train_labels.sum(axis=1) == 0, :].index\n",
    "train_labels = train_labels.drop(drop_y)\n",
    "train_dataset = train_dataset.drop(drop_y)\n",
    "\n",
    "# 과거 3개월에 대해 카드 이력이 없는 사람 제거\n",
    "x_labels = train_dataset[train_dataset.columns[0:20]].copy()\n",
    "drop_x = x_labels.loc[x_labels.sum(axis=1) == 0, :].index\n",
    "train_labels = train_labels.drop(drop_x)\n",
    "train_dataset = train_dataset.drop(drop_x)\n",
    "\n",
    "# 2. test set\n",
    "# 추후 1개월에 대해 카드 이력이 없는 사람을 제거\n",
    "drop_y = test_labels.loc[test_labels.sum(axis=1) == 0, :].index\n",
    "test_labels = test_labels.drop(drop_y)\n",
    "test_dataset = test_dataset.drop(drop_y)\n",
    "\n",
    "# 과거 3개월에 대해 카드 이력이 없는 사람 제거\n",
    "x_labels = test_dataset[test_dataset.columns[0:20]].copy()\n",
    "drop_x = x_labels.loc[x_labels.sum(axis=1) == 0, :].index\n",
    "test_labels = test_labels.drop(drop_x)\n",
    "test_dataset = test_dataset.drop(drop_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.shape # (11518, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연령대 범주화\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, np.inf]\n",
    "names = ['0', '10', '20', '30', '40', '50', '60', '70', '70']\n",
    "\n",
    "train_dataset['CLN_AGE_RANGE'] = pd.cut(train_dataset['CLN_AGE'], bins, labels=names)\n",
    "test_dataset['CLN_AGE_RANGE'] = pd.cut(test_dataset['CLN_AGE'], bins, labels=names)\n",
    "\n",
    "train_dataset['CLN_AGE_RANGE'] = train_dataset['CLN_AGE_RANGE'].astype(int)\n",
    "test_dataset['CLN_AGE_RANGE'] = test_dataaset['CLN_AGE_RANGE'].astype(int)\n",
    "train_dataset.dtypes\n",
    "train_age = train_dataset.pop('CLN_AGE')\n",
    "test_age = test_dataset.pop('CLN_AGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id 제거\n",
    "\n",
    "train_name = train_dataset.pop('key')\n",
    "test_name = test_dataset.pop('key')\n",
    "train_datset = train_dataset.drop(['CLNN'], axis=1)\n",
    "test_datset = test_dataset.drop(['CLNN'], axis=1)\n",
    "\n",
    "# 날짜 int로 변경\n",
    "train_dataset['date'] = train_dataset['date'].astype(str).str.slice(4, 6)\n",
    "test_dataset['date'] = test_dataset['date'].astype(str).str.slice(4, 6)\n",
    "train_dataset['date'] = train_dataset['date'].astype(int)\n",
    "test_dataset['date'] = test_dataset['date'].astype(int)\n",
    "\n",
    "# 범주변수 one hot encoding하는 함수\n",
    "\n",
    "def make_dummy(df):\n",
    "    colnames = []\n",
    "    for col in df.columns:\n",
    "        if ( df[col].dtype == object):\n",
    "            colnames.append(col)]\n",
    "    dataset = pd.get_dummies(df, columns=colnames, prefix=colnames)    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = make_dummy(train_dataset)\n",
    "test_dataset = make_dummy(test_dataset)\n",
    "\n",
    "test_dataset = test_dataset.drop(test_dataset.columns[26], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 제거\n",
    "# 'SAA_y'에 음수가 있어 0으로 변환\n",
    "train_dataset['SAA_y'] = train_dataset['SAA_y'].clip(lower=0)\n",
    "test_dataset['SAA_y'] = test_dataset['SAA_y'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_col = test_dataset.columns.difference(train_dataset.columns)\n",
    "test_dataset = test_dataset.drop(diff_col, 1)\n",
    "\n",
    "diff_col = train_dataset.columns.difference(test_dataset.columns)\n",
    "train_dataset = train_dataset.drop(diff_col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape # (135517, 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.shape # (11518, 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats = pd.DataFrame(train_stats.transpose())\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_stats.index)):\n",
    "    print(i, train_stats.index[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization for continuous variable\n",
    "\n",
    "for i in range(0, 40):\n",
    "    column = train_stats.index[1]\n",
    "    train_dataset.loc[:, column] = ( train_dataset.loc[:, column] - train_stats['mean'][i] ) / train_stats['std'][i]\n",
    "    test_dataset.loc[:, column] = ( test_dataset.loc[:, column] - test_stats['mean'][i] ) / test_stats['std'][i]\n",
    "    \n",
    "for i in [46, 47, 48]:\n",
    "    column = train_stats.index[i]\n",
    "    train_dataset.loc[:, column] = ( train_dataset.loc[:, column] - train_stats['mean'][i] ) / train_stats['std'][i]\n",
    "    test_dataset.loc[:, column] = ( test_dataset.loc[:, column] - test_stats['mean'][i] ) / test_stats['std'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = train_dataset\n",
    "test_scaled = test_dataset\n",
    "input_dataset = pd.DataFrame(train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(h1, h2, h3, 1r):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(h1, activation = tf.nn.relu, input_shape=[len(pd.DataFrame(input_dataset).keys())], \n",
    "                    bias_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=.05, seed=123) ),\n",
    "        layers.Dense(h2, activation = tf.nn.relu, bias_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=.05, seed=123)),\n",
    "        layers.Dense(h3, activation = tf.nn.relu, bias_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=.05, seed=123)),\n",
    "        layers.Dense(20, activation = 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop(r)\n",
    "    \n",
    "    model.compile(\n",
    "        loss ='binary_crossentropy',\n",
    "        optimizer = optimizer,\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
