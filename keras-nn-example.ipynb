{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import time\n",
    "\n",
    "PYTHONHASHSEED=0\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy 1.1.0\n",
      "numpy 1.13.3\n",
      "matplotlib 1.5.3\n",
      "pandas 0.23.0\n",
      "sklearn 0.19.1\n",
      "h5py 2.7.1\n",
      "tensorflow 1.10.0\n",
      "keras 2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import matplotlib\n",
    "import pandas\n",
    "import sklearn\n",
    "# import pydot\n",
    "import h5py\n",
    "\n",
    "import tensorflow\n",
    "# import keras\n",
    "\n",
    "print('scipy ' + scipy.__version__)\n",
    "print('numpy ' + numpy.__version__)\n",
    "print('matplotlib ' + matplotlib.__version__)\n",
    "print('pandas ' + pandas.__version__)\n",
    "print('sklearn ' + sklearn.__version__)\n",
    "print('h5py ' + h5py.__version__)\n",
    "\n",
    "print('tensorflow ' + tensorflow.__version__)\n",
    "print('keras ' + keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from pickle\n",
    "train_dataset = pd.read_pickle('.pkl')\n",
    "test_dataset = pd.read_pickle('.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.index = range(len(train_dataset.index))\n",
    "test_dataset.index = range(len(test_dataset.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 변환 및 기초작업\n",
    "# 결측치 중 nan 이면 0으로 대체\n",
    "train_dataset.iloc[:, 0:20] = train_dataset.iloc[:, 0:20].fillna(0)\n",
    "train_dataset.iloc[:, 53:73] = train_dataset.iloc[:, 53:73].fillna(0)\n",
    "\n",
    "# 남은 변수의 경우 NA 경우 제거\n",
    "train_dataset = train_dataset.dropna()\n",
    "\n",
    "# 동일작업 test set에도 실시\n",
    "test_dataset.iloc[:, 0:20] = test_dataset.iloc[:, 0:20].fillna(0)\n",
    "test_dataset.iloc[:, 53:73] = test_dataset.iloc[:, 53:73].fillna(0)\n",
    "test_dataset = test_dataset.dropna()\n",
    "\n",
    "# train_dataset.shape # (203728, 35)\n",
    "# test_dataset.shape  # (11518, 35)\n",
    "\n",
    "# label 과 feature 분리\n",
    "train_labels = train_dataset[train_dataset.columns[53:73]].copy()\n",
    "test_labels = test_dataset[test_dataset.columns[53:73]].copy()\n",
    "\n",
    "train_dataset = train_dataset.drop(train_dataset.columns[53:73], 1)\n",
    "test_dataset = test_dataset.drop(test_dataset.columns[53:73], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. train set\n",
    "# 추후 1개월에 대해 카드 이력이 없는 사람을 제거\n",
    "drop_y = train_labels.loc[train_labels.sum(axis=1) == 0, :].index\n",
    "train_labels = train_labels.drop(drop_y)\n",
    "train_dataset = train_dataset.drop(drop_y)\n",
    "\n",
    "# 과거 3개월에 대해 카드 이력이 없는 사람 제거\n",
    "x_labels = train_dataset[train_dataset.columns[0:20]].copy()\n",
    "drop_x = x_labels.loc[x_labels.sum(axis=1) == 0, :].index\n",
    "train_labels = train_labels.drop(drop_x)\n",
    "train_dataset = train_dataset.drop(drop_x)\n",
    "\n",
    "# 2. test set\n",
    "# 추후 1개월에 대해 카드 이력이 없는 사람을 제거\n",
    "drop_y = test_labels.loc[test_labels.sum(axis=1) == 0, :].index\n",
    "test_labels = test_labels.drop(drop_y)\n",
    "test_dataset = test_dataset.drop(drop_y)\n",
    "\n",
    "# 과거 3개월에 대해 카드 이력이 없는 사람 제거\n",
    "x_labels = test_dataset[test_dataset.columns[0:20]].copy()\n",
    "drop_x = x_labels.loc[x_labels.sum(axis=1) == 0, :].index\n",
    "test_labels = test_labels.drop(drop_x)\n",
    "test_dataset = test_dataset.drop(drop_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.shape # (11518, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연령대 범주화\n",
    "\n",
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, np.inf]\n",
    "names = ['0', '10', '20', '30', '40', '50', '60', '70', '70']\n",
    "\n",
    "train_dataset['CLN_AGE_RANGE'] = pd.cut(train_dataset['CLN_AGE'], bins, labels=names)\n",
    "test_dataset['CLN_AGE_RANGE'] = pd.cut(test_dataset['CLN_AGE'], bins, labels=names)\n",
    "\n",
    "train_dataset['CLN_AGE_RANGE'] = train_dataset['CLN_AGE_RANGE'].astype(int)\n",
    "test_dataset['CLN_AGE_RANGE'] = test_dataaset['CLN_AGE_RANGE'].astype(int)\n",
    "train_dataset.dtypes\n",
    "train_age = train_dataset.pop('CLN_AGE')\n",
    "test_age = test_dataset.pop('CLN_AGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id 제거\n",
    "\n",
    "train_name = train_dataset.pop('key')\n",
    "test_name = test_dataset.pop('key')\n",
    "train_datset = train_dataset.drop(['CLNN'], axis=1)\n",
    "test_datset = test_dataset.drop(['CLNN'], axis=1)\n",
    "\n",
    "# 날짜 int로 변경\n",
    "train_dataset['date'] = train_dataset['date'].astype(str).str.slice(4, 6)\n",
    "test_dataset['date'] = test_dataset['date'].astype(str).str.slice(4, 6)\n",
    "train_dataset['date'] = train_dataset['date'].astype(int)\n",
    "test_dataset['date'] = test_dataset['date'].astype(int)\n",
    "\n",
    "# 범주변수 one hot encoding하는 함수\n",
    "\n",
    "def make_dummy(df):\n",
    "    colnames = []\n",
    "    for col in df.columns:\n",
    "        if ( df[col].dtype == object):\n",
    "            colnames.append(col)]\n",
    "    dataset = pd.get_dummies(df, columns=colnames, prefix=colnames)    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = make_dummy(train_dataset)\n",
    "test_dataset = make_dummy(test_dataset)\n",
    "\n",
    "test_dataset = test_dataset.drop(test_dataset.columns[26], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상치 제거\n",
    "# 'SAA_y'에 음수가 있어 0으로 변환\n",
    "train_dataset['SAA_y'] = train_dataset['SAA_y'].clip(lower=0)\n",
    "test_dataset['SAA_y'] = test_dataset['SAA_y'].clip(lower=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_col = test_dataset.columns.difference(train_dataset.columns)\n",
    "test_dataset = test_dataset.drop(diff_col, 1)\n",
    "\n",
    "diff_col = train_dataset.columns.difference(test_dataset.columns)\n",
    "train_dataset = train_dataset.drop(diff_col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape # (135517, 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.shape # (11518, 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats = pd.DataFrame(train_stats.transpose())\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_stats.index)):\n",
    "    print(i, train_stats.index[i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization for continuous variable\n",
    "\n",
    "for i in range(0, 40):\n",
    "    column = train_stats.index[1]\n",
    "    train_dataset.loc[:, column] = ( train_dataset.loc[:, column] - train_stats['mean'][i] ) / train_stats['std'][i]\n",
    "    test_dataset.loc[:, column] = ( test_dataset.loc[:, column] - test_stats['mean'][i] ) / test_stats['std'][i]\n",
    "    \n",
    "for i in [46, 47, 48]:\n",
    "    column = train_stats.index[i]\n",
    "    train_dataset.loc[:, column] = ( train_dataset.loc[:, column] - train_stats['mean'][i] ) / train_stats['std'][i]\n",
    "    test_dataset.loc[:, column] = ( test_dataset.loc[:, column] - test_stats['mean'][i] ) / test_stats['std'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = train_dataset\n",
    "test_scaled = test_dataset\n",
    "input_dataset = pd.DataFrame(train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(h1, h2, h3, 1r):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(h1, activation = tf.nn.relu, input_shape=[len(pd.DataFrame(input_dataset).keys())], \n",
    "                    bias_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=.05, seed=123) ),\n",
    "        layers.Dense(h2, activation = tf.nn.relu, bias_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=.05, seed=123)),\n",
    "        layers.Dense(h3, activation = tf.nn.relu, bias_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=.05, seed=123)),\n",
    "        layers.Dense(20, activation = 'sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop(r)\n",
    "    \n",
    "    model.compile(\n",
    "        loss ='binary_crossentropy',\n",
    "        optimizer = optimizer,\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0 : print('')\n",
    "            print('.', end='')\n",
    "            \n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times=[]\n",
    "        \n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)\n",
    "        \n",
    "time_callback = TimeHistory()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(hist):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.plot(hist['epoch'], hist['acc'], label='Train sec')\n",
    "    plt.plot(hist['epoch'], hist['acc'], label='Val acc')\n",
    "    plt.legend()\n",
    "    plt.ylim([0.82, 0.9])\n",
    "    \n",
    "    \n",
    "def nn(h1, h2, h3, ep, 1r, bt):\n",
    "    EPCHS = ep\n",
    "    model = build_model(h1, h2, h3, 1r)\n",
    "    print(model.summary())\n",
    "    history = model.fit(input_dataset, train_labels, epochs=EPOCHS, batch_size=bt, \n",
    "                       validation_split=0.2, verbose=0, callbacks=[PrintDot(), time_callback])\n",
    "    times = time_callback.times\n",
    "    print(pd.DataFrame(times).sum() / 60)\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    print(hist.tail())\n",
    "    return model, hist\n",
    "\n",
    "def pracc(m):\n",
    "    model = m\n",
    "    loss, acc = model.evaluate(pd.DataFrame(test_scaled), test_labels, verbose=0)\n",
    "    test_prediction = model.predict(pd.DataFrmae(test_scaled))\n",
    "    pred = pd.DataFrame(test_prediction)\n",
    "    pred.columns = ['백화점', '대형마트', '', '', '', '...']\n",
    "    pred.index = test_name.astype(str).str.slice(0, 10)\n",
    "    pred = pd.DataFrame(pred)\n",
    "    cutoff = 0.5\n",
    "    pred[pred >= cutoff] = 1\n",
    "    pred[pred < cutoff] = 0\n",
    "    pred.columns = test_labels.columns\n",
    "    pred.index = test_labels.index\n",
    "    \n",
    "    B = []\n",
    "    \n",
    "    for col in pred.columns:\n",
    "        A = pd.concat([pred.loc[:, col], test_labels.loc[:, col]], axis=1)\n",
    "        A.columns = ['pred', 'true']\n",
    "        \n",
    "        tp = A.loc[A['pred'] == 1, :]\n",
    "        tp = tp['true'].sum()\n",
    "        tpfp = A['pred'].sum()\n",
    "        fp = tpfp - tp\n",
    "        \n",
    "        fn1 = A.loc[A['pred'] == 0, :]\n",
    "        fn = fn1['true'].sum()\n",
    "        tn = len(fn1) - fn\n",
    "        \n",
    "        B.append([col, tp, fp, fn, tn, (tp/(fp+tp)).round(3), (tp/(fn+tp)).round(3), ((tp+tn)/(tp+tn+fp+fn)).round(3) ])\n",
    "        \n",
    "    hit = pd.DataFrame(B, columns = ['column', 'TP', 'FP', 'FN', 'TN', 'Precision', 'Recall', 'Accuracy'])\n",
    "    model_hit = pd.DataFrame(hit.sum(axis=0)).transpose()\n",
    "    hit = model_hit.append(hit)\n",
    "    hit.iloc[0, 0] = 'model'\n",
    "    hit.iloc[0, 5] =  (hit.iloc[0, 1] / (hit.iloc[0, 1] + hit.iloc[0, 2] ))\n",
    "    hit.iloc[0, 6] =  (hit.iloc[0, 1] / (hit.iloc[0, 1] + hit.iloc[0, 3] ))\n",
    "    hit.iloc[0, 7] = ((hit.iloc[0, 1] + hit.iloc[0, 4]) / (hit.iloc[0, 1] + hit.iloc[0, 3] + hit.iloc[0, 2] + hit.iloc[0, 4]))\n",
    "    return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model\n",
    "\n",
    "model1, hist1 = nn(128, 128, 128, 200, 0.0001, 128)\n",
    "plot_history(hist1)\n",
    "print(pracc(model1))\n",
    "del[model1, hist1]\n",
    "\n",
    "model2, hist2 = nn(128, 64, 32, 200, 0.0001, 128)\n",
    "plot_history(hist2)\n",
    "print(pracc(model2))\n",
    "del[model2, hist2]\n",
    "\n",
    "model3, hist3 = nn(256, 128, 128, 200, 0.0001, 128)\n",
    "plot_history(hist3)\n",
    "print(pracc(model3))\n",
    "del[model3, hist3]\n",
    "\n",
    "model3, hist3 = nn(256, 256, 128, 200, 0.0001, 128)\n",
    "plot_history(hist3)\n",
    "print(pracc(model3))\n",
    "del[model3, hist3]\n",
    "\n",
    "model3, hist3 = nn(256, 256, 64, 200, 0.0001, 128)\n",
    "plot_history(hist3)\n",
    "print(pracc(model3))\n",
    "del[model3, hist3]\n",
    "\n",
    "model3, hist3 = nn(128, 64, 64, 200, 0.0001, 128)\n",
    "plot_history(hist3)\n",
    "print(pracc(model3))\n",
    "del[model3, hist3]\n",
    "\n",
    "model3, hist3 = nn(256, 64, 64, 200, 0.0001, 128)\n",
    "plot_history(hist3)\n",
    "print(pracc(model3))\n",
    "del[model3, hist3]\n",
    "\n",
    "model3, hist3 = nn(128, 128, 128, 200, 0.0001, 128)\n",
    "plot_history(hist3)\n",
    "print(pracc(model3))\n",
    "del[model3, hist3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3, hist3 = nn(128, 128, 64, 100, 0.0001, 128)\n",
    "plot_history(hist3)\n",
    "print(pracc(model3))\n",
    "del[model3, hist3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
